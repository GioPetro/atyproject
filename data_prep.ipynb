{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2060'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"owaiskha9654/PubMed_MultiLabel_Text_Classification_Dataset_MeSH\")\n",
    "dataset = dataset['train']\n",
    "df = pd.DataFrame(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>abstractText</th>\n",
       "      <th>meshMajor</th>\n",
       "      <th>pmid</th>\n",
       "      <th>meshid</th>\n",
       "      <th>meshroot</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Expression of p53 and coexistence of HPV in pr...</td>\n",
       "      <td>Fifty-four paraffin embedded tissue sections f...</td>\n",
       "      <td>['DNA Probes, HPV', 'DNA, Viral', 'Female', 'H...</td>\n",
       "      <td>8549602</td>\n",
       "      <td>[['D13.444.600.223.555', 'D27.505.259.750.600....</td>\n",
       "      <td>['Chemicals and Drugs [D]', 'Organisms [B]', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vitamin D status in pregnant Indian women acro...</td>\n",
       "      <td>The present cross-sectional study was conducte...</td>\n",
       "      <td>['Adult', 'Alkaline Phosphatase', 'Breast Feed...</td>\n",
       "      <td>21736816</td>\n",
       "      <td>[['M01.060.116'], ['D08.811.277.352.650.035'],...</td>\n",
       "      <td>['Named Groups [M]', 'Chemicals and Drugs [D]'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Identification of a functionally important di...</td>\n",
       "      <td>The occurrence of individual amino acids and d...</td>\n",
       "      <td>['Amino Acid Sequence', 'Analgesics, Opioid', ...</td>\n",
       "      <td>19060934</td>\n",
       "      <td>[['G02.111.570.060', 'L01.453.245.667.060'], [...</td>\n",
       "      <td>['Phenomena and Processes [G]', 'Information S...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multilayer capsules: a promising microencapsul...</td>\n",
       "      <td>In 1980, Lim and Sun introduced a microcapsule...</td>\n",
       "      <td>['Acrylic Resins', 'Alginates', 'Animals', 'Bi...</td>\n",
       "      <td>11426874</td>\n",
       "      <td>[['D05.750.716.822.111', 'D25.720.716.822.111'...</td>\n",
       "      <td>['Chemicals and Drugs [D]', 'Technology, Indus...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nanohydrogel with N,N'-bis(acryloyl)cystine cr...</td>\n",
       "      <td>Substantially improved hydrogel particles base...</td>\n",
       "      <td>['Antineoplastic Agents', 'Cell Proliferation'...</td>\n",
       "      <td>28323099</td>\n",
       "      <td>[['D27.505.954.248'], ['G04.161.750', 'G07.345...</td>\n",
       "      <td>['Chemicals and Drugs [D]', 'Phenomena and Pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Expression of p53 and coexistence of HPV in pr...   \n",
       "1  Vitamin D status in pregnant Indian women acro...   \n",
       "2  [Identification of a functionally important di...   \n",
       "3  Multilayer capsules: a promising microencapsul...   \n",
       "4  Nanohydrogel with N,N'-bis(acryloyl)cystine cr...   \n",
       "\n",
       "                                        abstractText  \\\n",
       "0  Fifty-four paraffin embedded tissue sections f...   \n",
       "1  The present cross-sectional study was conducte...   \n",
       "2  The occurrence of individual amino acids and d...   \n",
       "3  In 1980, Lim and Sun introduced a microcapsule...   \n",
       "4  Substantially improved hydrogel particles base...   \n",
       "\n",
       "                                           meshMajor      pmid  \\\n",
       "0  ['DNA Probes, HPV', 'DNA, Viral', 'Female', 'H...   8549602   \n",
       "1  ['Adult', 'Alkaline Phosphatase', 'Breast Feed...  21736816   \n",
       "2  ['Amino Acid Sequence', 'Analgesics, Opioid', ...  19060934   \n",
       "3  ['Acrylic Resins', 'Alginates', 'Animals', 'Bi...  11426874   \n",
       "4  ['Antineoplastic Agents', 'Cell Proliferation'...  28323099   \n",
       "\n",
       "                                              meshid  \\\n",
       "0  [['D13.444.600.223.555', 'D27.505.259.750.600....   \n",
       "1  [['M01.060.116'], ['D08.811.277.352.650.035'],...   \n",
       "2  [['G02.111.570.060', 'L01.453.245.667.060'], [...   \n",
       "3  [['D05.750.716.822.111', 'D25.720.716.822.111'...   \n",
       "4  [['D27.505.954.248'], ['G04.161.750', 'G07.345...   \n",
       "\n",
       "                                            meshroot  A  B  C  D  E  F  G  H  \\\n",
       "0  ['Chemicals and Drugs [D]', 'Organisms [B]', '...  0  1  1  1  1  0  0  1   \n",
       "1  ['Named Groups [M]', 'Chemicals and Drugs [D]'...  0  1  1  1  1  1  1  0   \n",
       "2  ['Phenomena and Processes [G]', 'Information S...  1  1  0  1  1  0  1  0   \n",
       "3  ['Chemicals and Drugs [D]', 'Technology, Indus...  1  1  1  1  1  0  1  0   \n",
       "4  ['Chemicals and Drugs [D]', 'Phenomena and Pro...  1  1  0  1  1  0  1  0   \n",
       "\n",
       "   I  J  L  M  N  Z  \n",
       "0  0  0  0  0  0  0  \n",
       "1  1  1  0  1  1  1  \n",
       "2  0  0  1  0  0  0  \n",
       "3  0  1  0  0  0  0  \n",
       "4  0  1  0  0  0  0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Article length:  192.05284\n",
      "Stdev Article length:  76.74764082329723\n",
      "Mesh Labels Root Class: \"\n",
      "\" ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']\n",
      "\n",
      "\n",
      "Number of Labels:  14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Root Label</th>\n",
       "      <th>number of Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>23263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>46577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>26453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>31074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>39202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F</td>\n",
       "      <td>8885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>G</td>\n",
       "      <td>33609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H</td>\n",
       "      <td>6069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I</td>\n",
       "      <td>5595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>J</td>\n",
       "      <td>5531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M</td>\n",
       "      <td>21363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>N</td>\n",
       "      <td>22919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Z</td>\n",
       "      <td>8049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Root Label  number of Abstract\n",
       "0           A               23263\n",
       "1           B               46577\n",
       "2           C               26453\n",
       "3           D               31074\n",
       "4           E               39202\n",
       "5           F                8885\n",
       "6           G               33609\n",
       "7           H                6069\n",
       "8           I                5595\n",
       "9           J                5531\n",
       "10          L                7503\n",
       "11          M               21363\n",
       "12          N               22919\n",
       "13          Z                8049"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Average Article length: ', df.abstractText.str.split().str.len().mean())\n",
    "print('Stdev Article length: ', df.abstractText.str.split().str.len().std())\n",
    "cols = df.columns\n",
    "cols = list(df.columns)\n",
    "mesh_Heading_categories = cols[6:]\n",
    "num_labels = len(mesh_Heading_categories)\n",
    "print('Mesh Labels Root Class: \"\\n\"',mesh_Heading_categories)\n",
    "print(\"\\n\")\n",
    "print('Number of Labels: ' ,num_labels)\n",
    "counts = []\n",
    "for mesh_Heading_category in mesh_Heading_categories:\n",
    "    counts.append((mesh_Heading_category, df[mesh_Heading_category].sum()))\n",
    "df_count = pd.DataFrame(counts, columns=['Root Label', 'number of Abstract'])\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 20)\n",
      "(10000, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>abstractText</th>\n",
       "      <th>meshMajor</th>\n",
       "      <th>pmid</th>\n",
       "      <th>meshid</th>\n",
       "      <th>meshroot</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>...</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>Z</th>\n",
       "      <th>one_hot_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>Mortality among residents of Uravan, Colorado ...</td>\n",
       "      <td>A cohort mortality study was conducted of all ...</td>\n",
       "      <td>['Adult', 'Causality', 'Cohort Studies', 'Colo...</td>\n",
       "      <td>17768330</td>\n",
       "      <td>[['M01.060.116'], ['N05.715.350.200', 'N06.850...</td>\n",
       "      <td>['Named Groups [M]', 'Health Care [N]', 'Analy...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>High frequency of inactivation of the imprinte...</td>\n",
       "      <td>Embryonal tumors such as Wilms' tumor (WT), em...</td>\n",
       "      <td>['Child', 'Child, Preschool', 'DNA Methylation...</td>\n",
       "      <td>10404060</td>\n",
       "      <td>[['M01.060.406'], ['M01.060.406.448'], ['G02.1...</td>\n",
       "      <td>['Named Groups [M]', 'Phenomena and Processes ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41779</th>\n",
       "      <td>Short-term clinical outcomes after HLA 1-locus...</td>\n",
       "      <td>In Japan, use of unrelated peripheral blood st...</td>\n",
       "      <td>['Acute Disease', 'Adolescent', 'Adult', 'Aged...</td>\n",
       "      <td>30877606</td>\n",
       "      <td>[['C23.550.291.125'], ['M01.060.057'], ['M01.0...</td>\n",
       "      <td>['Diseases [C]', 'Named Groups [M]', 'Phenomen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "2380   Mortality among residents of Uravan, Colorado ...   \n",
       "3385   High frequency of inactivation of the imprinte...   \n",
       "41779  Short-term clinical outcomes after HLA 1-locus...   \n",
       "\n",
       "                                            abstractText  \\\n",
       "2380   A cohort mortality study was conducted of all ...   \n",
       "3385   Embryonal tumors such as Wilms' tumor (WT), em...   \n",
       "41779  In Japan, use of unrelated peripheral blood st...   \n",
       "\n",
       "                                               meshMajor      pmid  \\\n",
       "2380   ['Adult', 'Causality', 'Cohort Studies', 'Colo...  17768330   \n",
       "3385   ['Child', 'Child, Preschool', 'DNA Methylation...  10404060   \n",
       "41779  ['Acute Disease', 'Adolescent', 'Adult', 'Aged...  30877606   \n",
       "\n",
       "                                                  meshid  \\\n",
       "2380   [['M01.060.116'], ['N05.715.350.200', 'N06.850...   \n",
       "3385   [['M01.060.406'], ['M01.060.406.448'], ['G02.1...   \n",
       "41779  [['C23.550.291.125'], ['M01.060.057'], ['M01.0...   \n",
       "\n",
       "                                                meshroot  A  B  C  D  ...  F  \\\n",
       "2380   ['Named Groups [M]', 'Health Care [N]', 'Analy...  0  1  1  1  ...  0   \n",
       "3385   ['Named Groups [M]', 'Phenomena and Processes ...  0  1  1  1  ...  0   \n",
       "41779  ['Diseases [C]', 'Named Groups [M]', 'Phenomen...  1  1  1  1  ...  0   \n",
       "\n",
       "       G  H  I  J  L  M  N  Z                              one_hot_labels  \n",
       "2380   0  0  1  1  0  1  1  1  [0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1]  \n",
       "3385   1  0  0  0  0  1  0  0  [0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0]  \n",
       "41779  1  0  0  0  0  1  1  0  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0]  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, random_state=17, test_size=0.20, shuffle=True)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "df_train['one_hot_labels'] = list(df_train[mesh_Heading_categories].values)\n",
    "\n",
    "labels = list(df_train.one_hot_labels.values)\n",
    "Article_train = list(df_train.abstractText.values)\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer,BertForSequenceClassification,BertTokenizer, RobertaForSequenceClassification,RobertaTokenizer\n",
    "\n",
    "max_length = 128\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base', do_lower_case=False)  # tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2', do_lower_case=True) \n",
    "#tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=False) \n",
    "\n",
    "encodings = tokenizer.batch_encode_plus(Article_train,max_length=max_length,padding=True,truncation=True) # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())\n",
    "\n",
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask'] # attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow me to stratify split our training data later\n",
    "label_counts = df_train.one_hot_labels.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(df_train[df_train.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\n",
    "# print('df label indices with only one instance: ', one_freq_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_17880\\84244288.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  train_labels = torch.tensor(train_labels)\n"
     ]
    }
   ],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "                                                            random_state=17, test_size=0.20)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our BERT Pytorch model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels,)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels,)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "torch.save(validation_dataloader,'validation_data_loader')\n",
    "torch.save(train_dataloader,'train_data_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Pushed to Cuda for Training\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=num_labels)\n",
    "# model = BertForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", num_labels=num_labels)\n",
    "# model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=num_labels)\n",
    "model.cuda()\n",
    "print('Model Pushed to Cuda for Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(optimizer_grouped_parameters,lr=6e-6)\n",
    "# optimizer = AdamW(model.parameters(),lr=4e-5)  # Default optimization #XL-NET\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH']='true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch :   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Train loss: 0.37973081463575364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch :  17%|█▋        | 1/6 [1:16:15<6:21:16, 4575.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Validation Accuracy:  81.17154811715481\n",
      "Flat Validation Accuracy:  9.375\n",
      "\n",
      "\n",
      "train_loss 0.37973081463575364 val_f1_accuracy 81.17154811715481 val_flat_accuracy 9.375\n",
      "cuda:0\n",
      "Train loss: 0.33233627837896346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch :  33%|███▎      | 2/6 [2:05:35<4:01:41, 3625.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Validation Accuracy:  81.46067415730337\n",
      "Flat Validation Accuracy:  14.0625\n",
      "\n",
      "\n",
      "train_loss 0.33233627837896346 val_f1_accuracy 81.46067415730337 val_flat_accuracy 14.0625\n",
      "cuda:0\n",
      "Train loss: 0.31424267506599424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch :  50%|█████     | 3/6 [3:00:30<2:53:43, 3474.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Validation Accuracy:  82.9817158931083\n",
      "Flat Validation Accuracy:  15.625\n",
      "\n",
      "\n",
      "train_loss 0.31424267506599424 val_f1_accuracy 82.9817158931083 val_flat_accuracy 15.625\n",
      "cuda:0\n",
      "Train loss: 0.3034516060352325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch :  67%|██████▋   | 4/6 [3:52:47<1:51:22, 3341.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Validation Accuracy:  83.49788434414668\n",
      "Flat Validation Accuracy:  15.625\n",
      "\n",
      "\n",
      "train_loss 0.3034516060352325 val_f1_accuracy 83.49788434414668 val_flat_accuracy 15.625\n",
      "cuda:0\n",
      "Train loss: 0.29469546088576315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch :  83%|████████▎ | 5/6 [4:43:36<53:55, 3235.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Validation Accuracy:  84.35374149659863\n",
      "Flat Validation Accuracy:  15.625\n",
      "\n",
      "\n",
      "train_loss 0.29469546088576315 val_f1_accuracy 84.35374149659863 val_flat_accuracy 15.625\n",
      "cuda:0\n",
      "Train loss: 0.28738275146484377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 100%|██████████| 6/6 [5:39:29<00:00, 3394.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Validation Accuracy:  84.9655172413793\n",
      "Flat Validation Accuracy:  17.1875\n",
      "\n",
      "\n",
      "train_loss 0.28738275146484377 val_f1_accuracy 84.9655172413793 val_flat_accuracy 17.1875\n",
      "CPU times: total: 1h 23min 27s\n",
      "Wall time: 5h 39min 29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import trange\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "# For Storing our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "val_f1_accuracy_list,val_flat_accuracy_list,training_loss_list,epochs_list=[],[],[],[]\n",
    "\n",
    "# Number of training epochs (recommend between 5 and 10)\n",
    "epochs = 6\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch \"):\n",
    "    # Training\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "    print(model.device)\n",
    "    # Tracking variables\n",
    "    tr_loss = 0 #running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass for multilabel classification\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "        # Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities\n",
    "        # Also This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable \n",
    "        # than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the \n",
    "        # log-sum-exp trick for numerical stability.\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        loss_func = BCEWithLogitsLoss()  \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "        train_loss_set.append(loss.item())    \n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    training_loss_list.append(tr_loss/nb_tr_steps)\n",
    "\n",
    "    ###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "    # Predict\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "        pred_label = pred_label.to('cpu').numpy()\n",
    "        b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tokenized_texts.append(b_input_ids)\n",
    "    logit_preds.append(b_logit_pred)\n",
    "    true_labels.append(b_labels)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "    # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)  \n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)\n",
    "    print('\\n')\n",
    "    val_f1_accuracy_list.append(val_f1_accuracy)\n",
    "    val_flat_accuracy_list.append(val_flat_accuracy)\n",
    "    epochs_list.append(epochs)  \n",
    "    \n",
    "    print(\"train_loss\", tr_loss/nb_tr_steps,\"val_f1_accuracy\", val_f1_accuracy,\"val_flat_accuracy\", val_flat_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"C:/Users/georg/Desktop/modelsss/roberta_pretrained_6epochs\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Optionally, save other configurations or metadata\n",
    "# For example, you can save the model configuration\n",
    "model.config.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b4dbafe67a4573bc67e8e1407b9387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673a3bdb59f14f39941f2870876379f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SuiGio/roberta_pubmesh/commit/e3328fd301657dcad5b770bb535afce349a3f8a5', commit_message='Upload tokenizer', commit_description='', oid='e3328fd301657dcad5b770bb535afce349a3f8a5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"SuiGio/roberta_pubmesh\")\n",
    "tokenizer.push_to_hub(\"SuiGio/roberta_pubmesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5720a4ebdbc64c019c366749fd49c6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65e8a789b92443aaefc6c09b73bf97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229ab2f3709d44478747476be3e89935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/506k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540e237a40f243cc8a4990a6cec27657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bff825fe7145e3b7fb4af66ec40092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c25b0017bc4dccb8bde29828035d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SuiGio/roberta_pubmesh\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SuiGio/roberta_pubmesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
